\chapter{Introduction}
\label{Chapter1}

The study of human behaviour is an important part of computer science, artificial intelligence (AI), neural computation, cognitive science, philosophy, psychology, and other areas. Presupposing that behaviour is generally determined and guided by cognition and mental processing, among the most prominent tools in the modelling of behaviour are computational-logic systems mostly addressing high-level reasoning and thought processes (classical logic, nonmonotonic logic, modal and temporal logic), connectionist models of cognition and the brain mostly addressing lower-level dynamics and emergent processes (feed-forward and recurrent networks, symmetric and deep networks, self-organising networks), and models of uncertainty addressing the often vague or probabilistic nature of many aspects of cognitive processing (Bayesian networks, Markov decision processes, Markov logic networks, probabilistic inductive logic programs).

The intuition motivating Neural-Symbolic integration as an active field of research is the following: in neural computing, it is assumed that the mind is an emergent property of the brain, and that computational cognitive modelling can lead to valid theories of cognition and offer an understanding of certain cognitive processes \cite{10.1016/j.cogsys.2008.07.002}. From this it is in turn assumed that connectionism should be able to offer an appropriate representational language for artificial intelligence as well. In particular, a connectionist computational theory of the mind should be able to replicate the parallelism and kinds of adaptive learning processes seen in neural networks, which are generally accepted as responsible for the necessary robustness and ultimate effectiveness of the system in dealing with commonsense knowledge. As a result, a purely symbolic approach would not be sufficient, as argued by Valiant in \cite{Valiant08knowledgeinfusion:}. On the other hand, logic is firmly established as a fundamental tool in modelling of thought and behaviour \cite{kowalski_2011} and by many has been viewed generally as the "calculus of computer science". However, when building models that combine learning and reasoning, one has to conciliate the methodologies of distinct areas (namely predominantly statistics and logic), in order to combine the respective advantages and circumvent the shortcomings and limitations.

From a more practical perspective, rational agents are often conceptualised as performing concept acquisition (generally unsepervised and statistical) and concept manipulation (generally supervised and symbolic) as a part of a permanent cycle of perception and action. The question of how to reconcile the statistical nature of learning with the logic nature of reasoning, aiming to build such robust computational models integrating concept acquisition and manipulation, has been identified as a key research challenge and fundamental problem in computer science \cite{10.1145/602382.602410}.

In this thesis, we  explore the integration between \textit{Sum-Product networks} (instead of classical artificial neural networks usually exploited in Neural-Symbolic models) and \textit{abductive reasoning}. A \textit{Sum-Product Network (SPN)} is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent univariate probability distributions and non-terminal nodes represent combinations (weighted sums) and products of probability functions. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of links in the graph. They are somewhat similar to neural networks and can address the same kinds of problems. In particular, we exploit \textit{Random-And-Tenzorized SPNs (RAT-SPNs)}, proposed by Peharz et al. \cite{DBLP:journals/corr/abs-1806-01910}, which are SPNs with random structures and trained in "classical deep learning manner", i.e. employing automatic differentiation, SGD and GPU support. On the other hand, \textit{abduction} is defined as "a syllogism in which the major premise is evident but the minor premise and therefore the conclusion only probabile". Basically, it involves forming a conclusion from the information that is known, broadly speaking trying to explain a puzzling observation. Abduction is entirely aimed at increase our knowledge, but brings the disadvantage to be extremely subjected to error, since it does not contain in itself its logic validity, but it has to be empirically validated (contrary to deduction). The integration between \textit{RAT-SPNs} and \textit{abduction} has been tested on a particular kind of classification task, that differs from the "standard" one because the known information about training examples is not directly their associated class but an aggregate information deriving from it. A trivial example might be learning to classify digits from 0 to 9 knowing the sum of \textit{n} pairs. It is clear that facing a task like this one leads to deal with a strong uncertainty component that must be handled by appropriate mechanisms.

The remainder of the thesis is structured as follows. In Chapter \ref{Chapter2}, we provide a brief introduction about Neural-Symbolic integration, Sum-Product Networks (and specifically RAT-SPNs), abductive reasoning and finally we formally define the classification task we deal with. The proposed model, its architecture and its main features are described in detail in Chapter \ref{Chapter3}, while Chapter \ref{ChapterExp} reports information about experiments setting and obtained results. In Chapter \ref{ChapterConclusions} we draw conclusions and outline future works. \\

\noindent \textbf{Keyword}: \keywordnames
