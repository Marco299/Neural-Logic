@article{10.1016/j.cogsys.2008.07.002,
	author = {Sun, Ron},
	title = {Theoretical Status of Computational Cognitive Modeling},
	year = {2009},
	issue_date = {June, 2009},
	publisher = {Elsevier Science Publishers B. V.},
	address = {NLD},
	volume = {10},
	number = {2},
	issn = {1389-0417},
	url = {https://doi.org/10.1016/j.cogsys.2008.07.002},
	doi = {10.1016/j.cogsys.2008.07.002},
	abstract = {This article explores the view that computational models of cognition may 		constitute valid theories of cognition, often in the full sense of the term ''theory''. In 	this discussion, this article examines various (existent or possible) positions on this issue and argues in favor of the view above. It also connects this issue with 	a  number of other relevant issues, such as the general relationship between theory and data, the validation of models, and the practical benefits of computational modeling. All the 	 discussions point to the position that computational cognitive models can be true theories of cognition.},
	journal = {Cogn. Syst. Res.},
	month = jun,
	pages = {124–140},
	numpages = {17},
	keywords = {Cognitive modeling, Theory, Simulation, Validation, Cognitive architecture}
}

@MISC{Valiant08knowledgeinfusion:,
    author = {Leslie G. Valiant},
    title = {Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence },
    year = {2008}
}

@book{kowalski_2011, 
	place={Cambridge}, 
	title={Computational Logic and Human Thinking: How to Be Artificially Intelligent}, 
	DOI={10.1017/CBO9780511984747}, 
	publisher={Cambridge University Press}, 
	author={Kowalski, Robert}, 
	year={2011}
}

@article{10.1145/602382.602410,
	author = {Valiant, Leslie G.},
	title = {Three Problems in Computer Science},
	year = {2003},
	issue_date = {January 2003},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {50},
	number = {1},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/602382.602410},
	doi = {10.1145/602382.602410},
	journal = {J. ACM},
	month = jan,
	pages = {96–99},
	numpages = {4}
}

@article{poon2011sum,
	author = {Hoifung Poon and Pedro M. Domingos},
	title = {Sum-Product Networks: A New Deep Architecture},
	journal = {CoRR},
	volume = {abs/1202.3732},
	year = {2012},
	url = {http://arxiv.org/abs/1202.3732},
	archivePrefix = {arXiv},
	eprint = {1202.3732},
	timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1202-3732.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}}

@inproceedings{Darwiche:2002fj,
  added-at = {2016-11-26T13:19:29.000+0100},
  author = {Darwiche, A.},
  biburl = {https://www.bibsonomy.org/bibtex/2bfc4fd4367c2e3b3e0929fc53b3e8859/machinelearning},
  booktitle = {Proceedings of KR},
  date-added = {2011-08-11 21:52:39 +0000},
  date-modified = {2011-08-11 21:53:23 +0000},
  interhash = {b029f0eb001d20fa94b72af68553bbeb},
  intrahash = {bfc4fd4367c2e3b3e0929fc53b3e8859},
  keywords = {imported ml},
  pages = {409--420},
  timestamp = {2016-11-26T13:20:49.000+0100},
  title = {A logical approach to factoring belief networks},
  year = 2002
}

@article{DBLP:journals/corr/abs-1301-3847,
  author    = {Adnan Darwiche},
  title     = {A Differential Approach to Inference in Bayesian Networks},
  journal   = {CoRR},
  volume    = {abs/1301.3847},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3847},
  archivePrefix = {arXiv},
  eprint    = {1301.3847},
  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3847.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{koller2009probabilistic,
  added-at = {2018-04-06T07:07:25.000+0200},
  author = {Koller, D. and Friedman, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2bfbf27ee9e268b4ad9666fdc925576a2/achakraborty},
  description = {Probabilistic Graphical Models: Principles and Techniques - Daphne Koller, Nir Friedman - Google Books},
  interhash = {0c61213fa4c06778a14fb33e04705fb5},
  intrahash = {bfbf27ee9e268b4ad9666fdc925576a2},
  isbn = {9780262013192},
  keywords = {2009 graph-theory machine-learning mit probability},
  lccn = {2009008615},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2018-04-06T07:07:25.000+0200},
  title = {Probabilistic Graphical Models: Principles and Techniques},
  url = {https://books.google.co.in/books?id=7dzpHCHzNQ4C},
  year = 2009
}

@InProceedings{pmlr-v28-gens13,
	  title = {Learning the Structure of Sum-Product Networks},
		author = {Robert Gens and Domingos Pedro},
		booktitle = {Proceedings of the 30th International Conference on Machine Learning},
		pages = {873--880},
		year = {2013},
		editor = {Sanjoy Dasgupta and David McAllester},
		volume = {28},
		number = {3},
		series = {Proceedings of Machine Learning Research},
		address = {Atlanta, Georgia, USA},
		month = {17--19 Jun},
		publisher = {PMLR},
		pdf = {http://proceedings.mlr.press/v28/gens13.pdf},
		url = {http://proceedings.mlr.press/v28/gens13.html},
		abstract = {Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.}
}

@InProceedings{pmlr-v32-rooshenas14,
	 title = {Learning Sum-Product Networks with Direct and Indirect Variable Interactions}, author = {Amirmohammad Rooshenas and Daniel Lowd},
	 booktitle = {Proceedings of the 31st International Conference on Machine Learning},
	 pages = {710--718},
	 year = {2014},
	 editor = {Eric P. Xing and Tony Jebara},
	 volume = {32},
	 number = {1},
	 series = {Proceedings of Machine Learning Research},
	 address = {Bejing, China},
	 month = {22--24 Jun},
	 publisher = {PMLR},
	 pdf = {http://proceedings.mlr.press/v32/rooshenas14.pdf},
	 url = {http://proceedings.mlr.press/v32/rooshenas14.html},
	 abstract = {Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference. SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures. Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables. In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables. In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.}
}

 @article{DBLP:journals/corr/abs-1710-03297,
   author    = {Alejandro Molina and
                Antonio Vergari and
                Nicola Di Mauro and
                Sriraam Natarajan and
                Floriana Esposito and
                Kristian Kersting},
   title     = {Sum-Product Networks for Hybrid Domains},
   journal   = {CoRR},
   volume    = {abs/1710.03297},
   year      = {2017},
   url       = {http://arxiv.org/abs/1710.03297},
   archivePrefix = {arXiv},
   eprint    = {1710.03297},
   timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
   biburl    = {https://dblp.org/rec/journals/corr/abs-1710-03297.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org}
 }

 @article{DBLP:journals/corr/DesanaS16,
   author    = {Mattia Desana and
                Christoph Schn{\"{o}}rr},
   title     = {Expectation Maximization for Sum-Product Networks as Exponential Family
                Mixture Models},
   journal   = {CoRR},
   volume    = {abs/1604.07243},
   year      = {2016},
   url       = {http://arxiv.org/abs/1604.07243},
   archivePrefix = {arXiv},
   eprint    = {1604.07243},
   timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
   biburl    = {https://dblp.org/rec/journals/corr/DesanaS16.bib},
   bibsource = {dblp computer science bibliography, https://dblp.org}
 }

@InProceedings{10.1007/978-3-319-23525-7_21,
   author="Vergari, Antonio
	 and Di Mauro, Nicola
	 and Esposito, Floriana",
	 editor="Appice, Annalisa
	 and Rodrigues, Pedro Pereira
	 and Santos Costa, V{\'i}tor
	 and Gama, Jo{\~a}o
	 and Jorge, Al{\'i}pio
	 and Soares, Carlos",
	 title="Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning",
	 booktitle="Machine Learning and Knowledge Discovery in Databases",
	 year="2015",
	 publisher="Springer International Publishing",
	 address="Cham",
	 pages="343--358",
	 abstract="The need for feasible inference in Probabilistic Graphical Models (PGMs) has lead to tractable models like Sum-Product Networks (SPNs). Their highly expressive power and their ability to provide exact and tractable inference make them very attractive for several real world applications, from computer vision to NLP. Recently, great attention around SPNs has focused on structure learning, leading to different algorithms being able to learn both the network and its parameters from data. Here, we enhance one of the best structure learner, LearnSPN, aiming to improve both the structural quality of the learned networks and their achieved likelihoods. Our algorithmic variations are able to learn simpler, deeper and more robust networks. These results have been obtained by exploiting some insights in the building process done by LearnSPN, by hybridizing the network adopting tree-structured models as leaves, and by blending bagging estimations into mixture creation. We prove our claims by empirically evaluating the learned SPNs on several benchmark datasets against other competitive SPN and PGM structure learners.",
	 isbn="978-3-319-23525-7"
}

@article{ProbCirc20,
  author    = {Choi, YooJung and Vergari, Antonio and Van den Broeck, Guy},
  title     = {Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models},
  month     = {sep},
  year      = {2020},
  url       = "http://starai.cs.ucla.edu/papers/ProbCirc20.pdf",
  keywords  = {techreport}
}

@inproceedings{choi2015tractable,
  title={Tractable learning for structured probability spaces: A case study in learning preference distributions},
  author={Choi, Arthur and Van den Broeck, Guy and Darwiche, Adnan},
  booktitle={Proceedings of 24th International Joint Conference on Artificial Intelligence (IJCAI)},
  volume={2015},
  pages={2861--2868},
  year={2015},
  organization={IJCAI-INT JOINT CONF ARTIF INTELL}
}

@inproceedings{khosravi2019tractable,
  title={On tractable computation of expected predictions},
  author={Khosravi, Pasha and Choi, YooJung and Liang, Yitao and Vergari, Antonio and Van den Broeck, Guy},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11169--11180},
  year={2019}
}

@inproceedings{liang2017towards,
  title={Towards compact interpretable models: Shrinking of learned probabilistic sentential decision diagrams},
  author={Liang, Yitao and Van den Broeck, Guy},
  booktitle={IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)},
  year={2017}
}

@article{DBLP:journals/corr/abs-1806-01910,
  author    = {Robert Peharz and
               Antonio Vergari and
               Karl Stelzner and
               Alejandro Molina and
               Martin Trapp and
               Kristian Kersting and
               Zoubin Ghahramani},
  title     = {Probabilistic Deep Learning using Random Sum-Product Networks},
  journal   = {CoRR},
  volume    = {abs/1806.01910},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.01910},
  archivePrefix = {arXiv},
  eprint    = {1806.01910},
  timestamp = {Mon, 24 Feb 2020 11:15:49 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-01910.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-642-40991-2_39,
author="Peharz, Robert
and Geiger, Bernhard C.
and Pernkopf, Franz",
editor="Blockeel, Hendrik
and Kersting, Kristian
and Nijssen, Siegfried
and {\v{Z}}elezn{\'y}, Filip",
title="Greedy Part-Wise Learning of Sum-Product Networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="612--627",
abstract="Sum-product networks allow to model complex variable interactions while still granting efficient inference. However, most learning algorithms proposed so far are explicitly or implicitly restricted to the image domain, either by assuming variable neighborhood or by assuming that dependent variables are related by their magnitudes over the training set. In this paper, we introduce a novel algorithm, learning the structure and parameters of sum-product networks in a greedy bottom-up manner. Our algorithm iteratively merges probabilistic models of small variable scope to larger and more complex models. These merges are guided by statistical dependence test, and parameters are learned using a maximum mutual information principle. In experiments our method competes well with the existing learning algorithms for sum-product networks on the task of reconstructing covered image regions, and outperforms these when neither neighborhood nor correlations by magnitude can be assumed.",
isbn="978-3-642-40991-2"
}

@article{DBLP:journals/corr/ZhaoMP15,
  author    = {Han Zhao and
               Mazen Melibari and
               Pascal Poupart},
  title     = {On the Relationship between Sum-Product Networks and Bayesian Networks},
  journal   = {CoRR},
  volume    = {abs/1501.01239},
  year      = {2015},
  url       = {http://arxiv.org/abs/1501.01239},
  archivePrefix = {arXiv},
  eprint    = {1501.01239},
  timestamp = {Fri, 08 Feb 2019 12:48:21 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ZhaoMP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/PeharzGPD16,
  author    = {Robert Peharz and
               Robert Gens and
               Franz Pernkopf and
               Pedro M. Domingos},
  title     = {On the Latent Variable Interpretation in Sum-Product Networks},
  journal   = {CoRR},
  volume    = {abs/1601.06180},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.06180},
  archivePrefix = {arXiv},
  eprint    = {1601.06180},
  timestamp = {Mon, 13 Aug 2018 16:47:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PeharzGPD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{srivastava2014dropout,
  added-at = {2017-01-28T20:14:22.000+0100},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  biburl = {https://www.bibsonomy.org/bibtex/24a1dac2f78b37c1f374a8228b0e272aa/hprop},
  interhash = {bdad866eb5fd8994c2aeae46af6def20},
  intrahash = {4a1dac2f78b37c1f374a8228b0e272aa},
  journal = {Journal of Machine Learning Research},
  keywords = {dropout machine-learning neural-networks regularization},
  number = 1,
  pages = {1929-1958},
  timestamp = {2017-01-28T20:31:22.000+0100},
  title = {Dropout: a simple way to prevent neural networks from overfitting.},
  url = {http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf},
  volume = 15,
  year = 2014
}

@book{Peirce1932,
author = "Charles S. Peirce",
title = "{The Collected Papers of Charles Sanders Peirce, Vol. II: Elements of Logic}",
editor = "Charles Hartshorne and Paul Weiss",
year = 1932,
address = "Cambridge",
publisher = "Harvard University Press",
language = "English",
note = "From the Commens Bibliography | \url{http://www.commens.org/bibliography/anthology_volume/peirce-charles-s-1932-collected-papers-charles-sanders-peirce-vol-ii}"
}

@inproceedings{Eshghi198801562579,
author = {Eshghi, Kave},
year = {1988},
month = {01},
pages = {562-579},
title = {Abductive Planning with Event Calculus.}
}

@Inbook{Kowalski1989,
author="Kowalski, Robert
and Sergot, Marek",
editor="Schmidt, Joachim W.
and Thanos, Constantino",
title="A Logic-Based Calculus of Events",
bookTitle="Foundations of Knowledge Base Management: Contributions from Logic, Databases, and Artificial Intelligence Applications",
year="1989",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="23--55",
abstract="We outline an approach for reasoning about events and time within a logic programming framework. The notion of event is taken to be more primitive than that of time and both are represented explicitly by means of Horn clauses augmented with negation by failure. The main intended applications are the updating of databases and narrative understanding. In contrast with conventional databases which assume that updates are made in the same order as the corresponding events occur in the real world, the explicit treatment of events allows us to deal with updates which provide new information about the past. Default reasoning on the basis of incomplete information is obtained as a consequence of using negation by failure. Default conclusions are automatically withdrawn if the addition of new information renders them inconsistent. Because events are differentiated from times, we can represent events with unknown times, as well as events which are partially ordered and concurrent.",
isbn="978-3-642-83397-7",
doi="10.1007/978-3-642-83397-7_2",
url="https://doi.org/10.1007/978-3-642-83397-7_2"
}

@inproceedings{Shanahan198901,
author = {Shanahan, Murray},
year = {1989},
month = {01},
pages = {1055-1060},
title = {Prediction is Deduction but Explanation is Abduction.}
}

@article{Denecker200112,
author = {Denecker, Marc and Missiaen, Lode and Bruynooghe, Maurice},
year = {2001},
month = {12},
pages = {},
title = {Temporal reasoning with Abductive Event Calculus}
}

@article{SHANAHAN2000207,
title = "An abductive event calculus planner",
journal = "The Journal of Logic Programming",
volume = "44",
number = "1",
pages = "207 - 240",
year = "2000",
issn = "0743-1066",
doi = "https://doi.org/10.1016/S0743-1066(99)00077-1",
url = "http://www.sciencedirect.com/science/article/pii/S0743106699000771",
author = "Murray Shanahan",
keywords = "Planning, Abductionl, Event calculus",
abstract = "In 1969 Cordell presented his seminal description of planning as theorem proving with the situation calculus. The most pleasing feature of Green's account was the negligible gap between high-level logical specification and practical implementation. This paper attempts to reinstate the ideal of planning via theorem proving in a modern guise. In particular, the paper shows that if we adopt the event calculus as our logical formalism and employ abductive logic programming as our theorem proving technique, then the computation performed mirrors closely that of a hand-coded partial-order planning algorithm. Soundness and completeness results for this logic programming implementation are given. Finally the paper shows that, if we extend the event calculus in a natural way to accommodate compound actions, then using the same abductive theorem proving techniques we can obtain a hierarchical planner."
}

@inproceedings{Kakas1998ACLPAC,
  title={ACLP: A case for Non-Monotonic Reasoning},
  author={A. Kakas and A. Michael and Costas Mourlas},
  year={1998}
}

@article{KAKAS2000129,
title = "ACLP: Abductive Constraint Logic Programming",
journal = "The Journal of Logic Programming",
volume = "44",
number = "1",
pages = "129 - 177",
year = "2000",
issn = "0743-1066",
doi = "https://doi.org/10.1016/S0743-1066(99)00075-8",
url = "http://www.sciencedirect.com/science/article/pii/S0743106699000758",
author = "A.C. Kakas and A. Michael and C. Mourlas",
keywords = "Abduction, Constraind solving",
abstract = "This paper presents the framework of Abductive Constraint Logic Programming (ACLP), which integrates Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving. This integration of constraint solving into abductive reasoning facilitates a general form of constructive abduction and enables the application of abduction to computationally demanding problems. The paper studies the formal declarative and operational semantics of the ACLP framework together with its application to various problems. The general characteristics of the computation of ACLP and of its application to problems are also discussed. Empirical results based on an implementation of the ACLP framework on top of the CLP language of ECLiPSe show that ACLP is computationally viable, with performance comparable to the underlying CLP framework on which it is built. In addition, our experiments show the natural ability for ACLP to accommodate easily and in a robust way new or changing requirements of the original problem. ACLP thus combines the advantages of modularity and flexibility of the high-level representation afforded by abduction together with the computational effectiveness of low-level specialised constraint solving."
}

@inproceedings{Eshghi198801234235,
author = {Eshghi, Kave and Kowalski, Robert},
year = {1989},
month = {01},
pages = {234-254},
title = {Abduction Compared with Negation by Failure.}
}

@inproceedings{Kakas199001,
author = {Kakas, Antonis and Mancarella, Paolo},
year = {1990},
month = {01},
pages = {650-661},
title = {Database Updates through Abduction.},
journal = {Proc. 16th VLDB}
}

@inproceedings{Kakas1990OnTR,
  title={On the relation between truth maintenance and abduction},
  author={A. Kakas and P. Mancarella},
  year={1990}
}

@inproceedings{Dung1991NegationsAH,
  title={Negations as Hypotheses: An Abductive Foundation for Logic Programming},
  author={P. Dung},
  booktitle={ICLP},
  year={1991}
}

@article{10.1093/logcom/5.5.579,
    author = {MISSIAEN, LODE and BRUYNOOGHE, MAURICE and DENECKER, MARC},
    title = "{CHICA, An Abductive Planning System Based on Event Calculus}",
    journal = {Journal of Logic and Computation},
    volume = {5},
    number = {5},
    pages = {579-602},
    year = {1995},
    month = {10},
    abstract = "{This article presents the theory and implementation of an artificial intelligence planner, CHICA. CHICA is a non-linear, domain independent planner based on techniques of computational logic. The representation language of the planner is Horn clause logic which is used to model event calculus, a logical theory of changing properties over time. The reasoning component is an abductive extension of SLDNF resolution for generating assumptions to prove a given goal. In event calculus, this procedure generates a plan of events and temporal relations necessary to prove the planning goal. CHICA uses domain contraints and techniques from contraint logic programming to efficiently implement inequality, as well as a specialized module to evaluate temporal relations. CHICA's generic search algorithm lets the implementor of a planning domain define a particular search strategy and specify domain heuristics to prune the search space. CHICA has solved a number of planning problems successfully: multiple robot block world problems, the assembly of a flashlight, and a room decoration problem. Extensions to classical Al-planning can be solved within the same framework, such as plan execution and replanning.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/5.5.579},
    url = {https://doi.org/10.1093/logcom/5.5.579},
    eprint = {https://academic.oup.com/logcom/article-pdf/5/5/579/3081061/5-5-579.pdf},
}

@article{POOLE198827,
title = "A logical framework for default reasoning",
journal = "Artificial Intelligence",
volume = "36",
number = "1",
pages = "27 - 47",
year = "1988",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(88)90077-X",
url = "http://www.sciencedirect.com/science/article/pii/000437028890077X",
author = "David Poole",
abstract = "This paper presents a simple logical framework for default reasoning. The semantics is normal first-order model theory; instead of changing the logic, the way in which the logic is used is changed. Rather than expecting reasoning to be just deduction (in any logic) from our knowledge, we examine the consequences of viewing reasoning as a very simple case of theory formation. By treating defaults as predefined possible hypotheses we show how this idea subsumes the intuition behind Reiter's default logic. Solutions to multiple extension problems are discussed. A prototype implementation, called THEORIST, executes all of the examples given."
}

@article{Denecker2001125,
author = {Denecker, Marc and De Schreye, Danny},
year = {2001},
month = {12},
pages = {},
title = {Representing Incomplete Knowledge in Abductive Logic Programming},
volume = {5},
journal = {Journal of Logic and Computation},
doi = {10.1093/logcom/5.5.553}
}

@InProceedings{10.1007/3-540-59487-6_2,
author="Denecker, Marc",
editor="Marek, V. Wiktor
and Nerode, Anil
and Truszczy{\'{n}}ski, M.",
title="A terminological interpretation of (abductive) logic programming",
booktitle="Logic Programming and Nonmonotonic Reasoning",
year="1995",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="15--28",
abstract="The logic program formalism is commonly viewed as a modal or default logic. In this paper, we propose an alternative interpretation of the formalism as a terminological logic. A terminological logic is designed to represent two different forms of knowledge. A TBox represents definitions for a set of concepts. An ABox represents the assertional knowledge of the expert. In our interpretation, a logic program is a TBox providing definitions for all predicates; this interpretation is present already in Clark's completion semantics. We extend the logic program formalism such that some predicates can be left undefined and use classical logic as the language for the ABox. The resulting logic can be seen as an alternative interpretation of abductive logic program formalism. We study the expressivity of the formalism for representing uncertainty by proposing solutions for problems in temporal reasoning, with null values and open domain knowledge.",
isbn="978-3-540-49282-5"
}

@article{10.1093/logcom/2.6.719,
    author = {KAKAS, A. C. and KOWALSKI, R. A. and TONI, F.},
    title = "{Abductive Logic Programming}",
    journal = {Journal of Logic and Computation},
    volume = {2},
    number = {6},
    pages = {719-770},
    year = {1992},
    month = {12},
    abstract = "{This paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning (abductive logic programming). We outline the general framework of abduction and its applications to knowledge assimilation and default reasoning; and we introduce an argumentation-theoretic approach to the use of abduction as an interpretation for negation as failure. We also analyse the links between abduction and the extension of logic programming obtained by adding a form of explicit negation. Finally we discuss the relation between abduction and truth maintenance.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/2.6.719},
    url = {https://doi.org/10.1093/logcom/2.6.719},
    eprint = {https://academic.oup.com/logcom/article-pdf/2/6/719/2776092/2-6-719.pdf},
}

@MISC{Kakas98therole,
    author = {A. C. Kakas and R. A. Kowalski and F. Toni},
    title = {The Role of Abduction in Logic Programming},
    year = {1998}
}

@book{10.5555/343643,
author = {Siegelmann, Hava T.},
title = {Neural Networks and Analog Computation: Beyond the Turing Limit},
year = {1999},
isbn = {0817639497},
publisher = {Birkhauser Boston Inc.},
address = {USA}
}

@article{Rooij2008099398432,
author = {Rooij, Iris},
year = {2008},
month = {09},
pages = {939-84},
title = {The Tractable Cognition Thesis},
volume = {32},
journal = {Cognitive science},
doi = {10.1080/03640210801897856}
}

@article{Leitgeb200508189202146,
author = {Leitgeb, Hannes},
year = {2005},
month = {08},
pages = {189-202},
title = {Interpreted Dynamical Systems and Qualitative Laws: from Neural Networks to Evolutionary Systems},
volume = {146},
journal = {Synthese},
doi = {10.1007/s11229-005-9086-5}
}

@article{10.1162/neco.2006.18.7.1527,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
title = {A Fast Learning Algorithm for Deep Belief Nets},
year = {2006},
issue_date = {July 2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {18},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2006.18.7.1527},
doi = {10.1162/neco.2006.18.7.1527},
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
journal = {Neural Comput.},
month = jul,
pages = {1527–1554},
numpages = {28}
}

@article{Pinkas1991062822913,
author = {Pinkas, Gadi},
year = {1991},
month = {06},
pages = {282-291},
title = {Symmetric Neural Networks and Propositional Logic Satisfiability},
volume = {3},
journal = {Neural Computation - NECO},
doi = {10.1162/neco.1991.3.2.282}
}

@inproceedings{dePenning201001,
author = {de Penning, Leo},
year = {2010},
month = {01},
pages = {},
title = {An Integrated Neural Symbolic Cognitive Agent Architecture for Training and Assessment in Simulators}
}

@inproceedings{10.5555/1597148.1597205,
author = {Garcez, Artur S. d'Avila and Gabbay, Dov M.},
title = {Fibring Neural Networks},
year = {2004},
isbn = {0262511835},
publisher = {AAAI Press},
abstract = {Neural-symbolic systems are hybrid systems that integrate symbolic logic and neural networks. The goal of neural-symbolic integration is to benefit from the combination of features of the symbolic and connectionist paradigms of artificial intelligence. This paper introduces a new neural network architecture based on the idea of fibring logical systems. Fibring allows one to combine different logical systems in a principled way. Fibred neural networks may be composed not only of interconnected neurons but also of other networks, forming a recursive architecture. A fibring function then defines how this recursive architecture must behave by defining how the networks in the ensemble relate to each other, typically by allowing the activation of neurons in one network (A) to influence the change of weights in another network (B). Intuitively, this can be seen as training network B at the same time that one runs network A. We show that, in addition to being universal approximators like standard feedforward networks, fibred neural networks can approximate any polynomial function to any desired degree of accuracy, thus being more expressive than standard feedforward networks.},
booktitle = {Proceedings of the 19th National Conference on Artifical Intelligence},
pages = {342–347},
numpages = {6},
keywords = {neural-symbolic integration, recursion, fibring systems},
location = {San Jose, California},
series = {AAAI'04}
}

@inproceedings{10.5555/3327144.3327291,
author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and Raedt, Luc De},
title = {DeepProbLog: Neural Probabilistic Logic Programming},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and sub-symbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {3753–3763},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@book{10.5555/2380985,
author = {Murphy, Kevin P.},
title = {Machine Learning: A Probabilistic Perspective},
year = {2012},
isbn = {0262018020},
publisher = {The MIT Press},
abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students. }
}

@book{10.5555/3161223,
author = {Buduma, Nikhil and Locascio, Nicholas},
title = {Fundamentals of Deep Learning: Designing Next-Generation Machine Intelligence Algorithms},
year = {2017},
isbn = {1491925612},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {With the reinvigoration of neural networks in the 2000s, deep learning has become an extremely active area of research, one thats paving the way for modern machine learning. In this practical book, author Nikhil Buduma provides examples and clear explanations to guide you through major concepts of this complicated field. Companies such as Google, Microsoft, and Facebook are actively growing in-house deep-learning teams. For the rest of us, however, deep learning is still a pretty complex and difficult subject to grasp. If youre familiar with Python, and have a background in calculus, along with a basic understanding of machine learning, this book will get you started. Examine the foundations of machine learning and neural networks Learn how to train feed-forward neural networks Use Tensor Flow to implement your first neural network Manage problems that arise as you begin to make networks deeper Build neural networks that analyze complex images Perform effective dimensionality reduction using autoencoders Dive deep into sequence analysis to examine language Understand the fundamentals of reinforcement learning}
}

@inproceedings{macqueen1967,
address = "Berkeley, Calif.",
author = "MacQueen, J.",
booktitle = "Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics",
pages = "281--297",
publisher = "University of California Press",
title = "Some methods for classification and analysis of multivariate observations",
url = "https://projecteuclid.org/euclid.bsmsp/1200512992",
year = "1967"
}

@article{1056489,
  author={S. {Lloyd}},
  journal={IEEE Transactions on Information Theory}, 
  title={Least squares quantization in PCM}, 
  year={1982},
  volume={28},
  number={2},
  pages={129-137},
  doi={10.1109/TIT.1982.1056489}}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
